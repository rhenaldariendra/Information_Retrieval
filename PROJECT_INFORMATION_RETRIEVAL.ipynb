{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PROJECT_INFORMATION_RETRIEVAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fi64aA0FFxcS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d44837-2fde-41ff-86ce-39af4796394d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-6aCmyB1k9Q",
        "outputId": "95d6fc80-e225-4b2a-84a9-ea7d29343f63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.43.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from attention import AttentionLayer"
      ],
      "metadata": {
        "id": "MDB0MFQH0Tw7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "_Jpu8qLEFxcY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Dataset from kaggle : https://www.kaggle.com/sunnysai12345/news-summary\n",
        "\n",
        "After reading the dataset we store it into DataFrame from pandas library"
      ],
      "metadata": {
        "id": "xxR0lXTX4Dj3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wnK5o4Z1Fxcj"
      },
      "source": [
        "data=pd.read_csv(\"Dataset/news_summary.csv\",engine='python',nrows=100000)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGNQKvCaISIn"
      },
      "source": [
        "# Drop Duplicates and NA values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Cjul88oOFxcr"
      },
      "source": [
        "data.drop_duplicates(subset=['text'],inplace=True)#dropping duplicates\n",
        "data.dropna(axis=0,inplace=True)#dropping na"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "__fy-JxTFxc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd791e3d-12fa-4fbe-b746-a8e0574b7f58"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4396 entries, 0 to 4513\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   author     4396 non-null   object\n",
            " 1   date       4396 non-null   object\n",
            " 2   headlines  4396 non-null   object\n",
            " 3   read_more  4396 non-null   object\n",
            " 4   text       4396 non-null   object\n",
            " 5   ctext      4396 non-null   object\n",
            "dtypes: object(6)\n",
            "memory usage: 240.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0s6IY-x2FxdL"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n",
        "\n",
        "Pada Data Cleaning ini kelompok kami akan menggunakan metode Regular Expressions dari library 're', dengan menggunakan dasar dari string manipulation. Dimana pada manipulasi string ini akan membantu membuat data yang dikerjakan lebih konsisten karena setiap dataset yang kita dapat terdiri dari text yang tidak tersturktur atau unstructured-text."
      ],
      "metadata": {
        "id": "zQqjJ8tT4Nh5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XZr-u3OEFxdT"
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:                                                 #removing short word\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "A2QAeCHWFxdY"
      },
      "source": [
        "#call the function\n",
        "cleaned_text = []\n",
        "for t in data['text']:\n",
        "    cleaned_text.append(text_cleaner(t,0)) "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GsRXocxoFxd-"
      },
      "source": [
        "#call the function\n",
        "cleaned_summary = []\n",
        "for t in data['headlines']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "L1zLpnqsFxey"
      },
      "source": [
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT_D2cLiLy77"
      },
      "source": [
        "#Drop empty rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sYK390unFxfA"
      },
      "source": [
        "data.replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rare Word Analysis\n",
        "Pada Code dibawah ini data akan dieprsiapkan untuk di fit atau di train dalam model menggunakan metode sequence to sequence. Diketahui bahwa proses dari sequence to sequence menggunakan yang namanya Encoder dan Decoder dimana merupakan bagian dari model LSTM, yang dimana nantinya pada output yang diberikan akan ada 2 macam token yang menandakan awalan dan akhiran dari output. pada case ini kami menggunakan token '_START_' dan '_END_'.\n",
        "\n",
        "Pada proses rare word analysis ini akan dihitung jumlah vocab yang ada pada text dan membandingkannya dengan threshold yang ditentukan. Dan juga akan dicari jumlah common words atau kata kata yang paling sering muncul"
      ],
      "metadata": {
        "id": "HFheBiXz4cOe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MdF76AHHFxgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "b1ff196a-2639-40e5-d908-c8ea68335feb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in data['cleaned_text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in data['cleaned_summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcFUlEQVR4nO3df5BdZZ3n8ffHBAEjkkCwxYSZMEMKizEqkAUc3LGXKIbAEv4AhWE1YLZSbsEMLlkl6O4wjrobdhYRHBcrZRjAAQKDIlnFkQzQxVq7BAm/wi+XhgkmXYHIjwQDorZ+94/zXLi5fbv7dve995x++vOqutXnPOfH/Z57Tn/76eee5zyKCMzMLC9vKTsAMzNrPyd3M7MMObmbmWXIyd3MLENO7mZmGXJyNzPLkJO7mVmGnNxLJGmLpI9UZT9mlg8ndzObEiRNLzuGbnJyL4mk7wB/APwvSbslfV7ScZL+j6Sdkh6W1JvW/VNJL0g6JM2/X9LLkt7TbD+lHZRlT9JFkgYk/VLSzyQtknSNpK/UrdMraVvd/BZJn5P0iKRXJa2V1CPpR2k//yxpVlp3nqSQdK6krek6/4ykf5W23ynp7+r2/ceS7pL0YvoduV7SzIb3vkjSI8CrKY7vNhzTlZKu6OgHV4aI8KukF7AF+EiangO8CCyh+KP70TR/UFr+VeAuYF9gM3B+s/345VenXsDhwFbg3Wl+HvDHwDXAV+rW6wW21c1vAe4FetJ1vgN4ADgS2Cdd15fU7TOAb6VlJwKvA98H3lm3/YfT+oel35W9gYOAe4CvN7z3Q8Ah6XfnYOBVYGZaPj3t7+iyP992v1xzr45/B9weEbdHxO8jYgNwP0WyB/hrYH/gPmAA+GYpUdpU9juKJHqEpL0iYktEPN3itt+IiOcjYgD438DGiHgwIl4HbqVI9PW+HBGvR8QdFMn4xojYUbf9kQAR0R8RGyLi1xHxC+BrwIcb9nVlRGyNiF9FxHaKPwBnpGWLgRciYtOYPolJwMm9Ov4QOCP927lT0k7gQxQ1DSLitxQ1pPcCl0Wqdph1S0T0A5+lqGjskLRO0rtb3Pz5uulfNZl/+3jWT80761JT0SvAPwCzG/a1tWH+WorKFOnnd1o8hknFyb1c9Ql6K/CdiJhZ95oREasBJM0BLgH+HrhM0t7D7MesYyLihoj4EEVlJIBLKWrWb6tb7V1dDOm/pjgWRMQ7KJK1GtZp/P34PvA+Se8FTgGu73iUJXByL9fzwB+l6X8A/q2kj0maJmmf9MXUXEmiqLWvBZYD24EvD7Mfs46QdLikE1LF4nWKGvTvKdq0l0g6QNK7KGr33bIfsBvYlSpAnxttg9QUdAtwA3BfRPy8syGWw8m9XP8N+M+pCeYTwFLgC8AvKGryn6M4R39J8WXSf0nNMecC50r61437kfSfunwMNnXsDawGXgCeo7gmL6Zo1niY4svLO4CbuhjTl4CjgF3AD4HvtbjdtcACMm2SAZCbbs1sqpH0B8CTwLsi4pWy4+kE19zNbEqR9BbgQmBdrokdins8zcymBEkzKL6jepbiNshsuVnGzCxDbpYxM8tQJZplZs+eHfPmzSs7jDF79dVXmTFjRtlhTEhOx7Bp06YXIuKgsuNpxUjXfBXPSdViqlo8UE5MI17zZT//ICI4+uijYzK6++67yw5hwnI6BuD+qMD13MprpGu+iuekajFVLZ6IcmIa6Zp3s4yZWYac3M3MMuTkbmaWoZaSe3rg/WZJD0m6P5UdIGmDpKfSz9rD9pUeft+fHq5/VCcPwMzMhhpLzf3fRMQHImJhml8F3BkR84E70zzAScD89FoBXNWuYM3MrDUTaZZZSvHwHdLP0+rKr0tf5t4LzJR08ATex8zMxqjV5B7AHZI2SVqRynqiGNUEiifE9aTpOez5cPxtqczMzLqk1U5MH4qIAUnvBDZIerJ+YUSEpDE9xyD9kVgB0NPTQ19f31g2r4Tdu3dPyrjr+RjM8tRSco9i3EIiYoekW4FjgOclHRwR21Ozy460+gDFYLQ1c1NZ4z7XAGsAFi5cGL29veM+iLL09fUxGeOu52Mwy9OoyT09Re0tEfHLNH0i8DfAemAZxcP7lwG3pU3WA+dLWgccC+yqa76xJuat+uEe81tWn1xSJGbd0XjNg6/7dmul5t4D3FqM9MZ04IaI+CdJPwVulrSc4vGZH0/r3w4sAfqB1yhGDTIzsy4aNblHxDPA+5uUvwgsalIewHltic7MzMbFPVTNzDLk5G5mliEndzOzDDm5m5llqBIjMdmefJuYmU2Ua+5mZhlycjczy5CTu5lZhpzczcwy5ORuU5akqyXtkPRoXdnfSnoyjSJ2q6SZdcsuTiOM/UzSx+rKF6eyfkmrGt/HrAxO7jaVXQMsbijbALw3It4H/D/gYgBJRwBnAn+StvmfkqZJmgZ8k2IEsiOAs9K6ZqVycrcpKyLuAV5qKLsjIgbT7L0Uj6yGYoSxdRHx64j4F4oH4x2TXv0R8UxE/AZYl9Y1K5Xvczcb3qeBm9L0HIpkX1M/wljjyGPHNttZqwPUVHHwkXbHtHLB4JCysex/KnxGE+XkXoJmnZSsWiR9ERgErm/XPlsdoKaKg4+0O6ZzmnXUO7v1/U+Fz2iinNzNGkg6BzgFWJQeYQ0jjzA26shjZt3m5D5JeLSm7pC0GPg88OGIeK1u0XrgBklfA94NzAfuAwTMl3QoRVI/E/jz7kZtNpSTu01Zkm4EeoHZkrYBl1DcHbM3xUDwAPdGxGci4jFJNwOPUzTXnBcRv0v7OR/4MTANuDoiHuv6wZg1cHK3KSsizmpSvHaE9b8KfLVJ+e0Uw0uaVYZvhTQzy5CTu5lZhpzczcwy5ORuZpYhJ3czsww5uZuZZcjJ3cwsQ07uZmYZcnI3M8uQk7uZWYac3M3MMuTkbmaWISd3M7MMObmbmWXIyd3MLEMtP89d0jTgfmAgIk5JI8+sAw4ENgGfjIjfSNobuA44GngR+EREbGl75FNcs3FYPTqTmdWMpeZ+AfBE3fylwOURcRjwMrA8lS8HXk7ll6f1zMwmbN6qHzJv1Q/ZPLDLA82PoqXkLmkucDLw7TQv4ATglrTKtcBpaXppmictX5TWNzOzLmm1WebrFIMG75fmDwR2RsRgmt8GzEnTc4CtABExKGlXWv+F+h1KWgGsAOjp6aGvr2+ch1Ce3bt3jyvulQsGR19pHMYTy3iPoUpyOAazdhs1uUs6BdgREZsk9bbrjSNiDbAGYOHChdHb27Zdd01fXx/jifucDv07ueXs3jFvM95jqJIcjsGs3VqpuR8PnCppCbAP8A7gCmCmpOmp9j4XGEjrDwCHANskTQf2p/hi1czMumTUNveIuDgi5kbEPOBM4K6IOBu4Gzg9rbYMuC1Nr0/zpOV3RUS0NWozMxvRRO5zvwi4UFI/RZv62lS+FjgwlV8IrJpYiGZmNlZjSu4R0RcRp6TpZyLimIg4LCLOiIhfp/LX0/xhafkznQjcbKIkXS1ph6RH68oOkLRB0lPp56xULklXSuqX9Iiko+q2WZbWf0rSsmbvZdZt7qFqU9k1wOKGslXAnRExH7iTN//zPAmYn14rgKug+GMAXAIcCxwDXFL7g2BWJid3m7Ii4h7gpYbi+n4ajf03rovCvRQ3FBwMfAzYEBEvRcTLwAaG/sEw67qWHz9gNkX0RMT2NP0c0JOm3+i/kdT6dgxXPkSrfTuqeN9+u2Nq1tejlf3XtuvZt5iu0udUtfPm5G42jIgISW2706vVvh1VvG+/3TE16+vRSj+N2nYrFwxy2ebp4+rb0SlVO29uljHb0/OpuYX0c0cqr/XfqKn17Riu3KxUTu5me6rvp9HYf+NT6a6Z44Bdqfnmx8CJkmalL1JPTGVmpXKzjE1Zkm4EeoHZkrZR3PWyGrhZ0nLgWeDjafXbgSVAP/AacC5ARLwk6cvAT9N6fxMRjV/SmnWdk7tNWRFx1jCLFjVZN4DzhtnP1cDVbQzNbMLcLGNmliHX3M1sQhoHzfCIYNXgmruZWYac3M3MMuTkbmaWISd3M7MM+QvVDvMI7WZWBtfczcwy5ORuZpYhJ3czsww5uZuZZcjJ3cwsQ07uZmYZcnI3M8uQk7uZWYac3M3MMuTkbmaWISd3M7MMObmbmWXIyd3MLENO7mZmGXJyNzPLkJO7mVmGnNzNmpD0HyU9JulRSTdK2kfSoZI2SuqXdJOkt6Z1907z/Wn5vHKjN3NyNxtC0hzgL4GFEfFeYBpwJnApcHlEHAa8DCxPmywHXk7ll6f1zEo1anJPNZb7JD2cajJfSuWuxVjOpgP7SpoOvA3YDpwA3JKWXwuclqaXpnnS8kWS1MVYzYZoZQzVXwMnRMRuSXsBP5H0I+BCilrMOknfoqi9XEVdLUZSrbbziQ7Fb9Z2ETEg6X8APwd+BdwBbAJ2RsRgWm0bMCdNzwG2pm0HJe0CDgReqN+vpBXACoCenh76+vqavv/u3buHXVaWkWJauWBwj/lWYm/cZqzb9exbTFfpc6raeRs1uUdEALvT7F7pFRS1mD9P5dcCf02R3JemaShqMX8nSWk/ZpUnaRbFdXwosBP4R2DxRPcbEWuANQALFy6M3t7epuv19fUx3LKyjBTTOQ2DwG85u/l6I20z1u1WLhjkss3TW9qmW6p23lqpuSNpGkXN5TDgm8DTdKkWU2Wt/KVuVkPplPF8hlWrbYxHB47hI8C/RMQvACR9DzgemClperru5wIDaf0B4BBgW2rG2R94sZ0BmY1VS8k9In4HfEDSTOBW4D0TfeNWazFV1spf6mY1lE4ZTy2marWN8ejAMfwcOE7S2yiaZRYB9wN3A6cD64BlwG1p/fVp/v+m5Xf5P1Ur25julomInRQX+AdJtZi0qFktBtdibDKKiI0UTYoPAJspfk/WABcBF0rqp/hvdG3aZC1wYCq/EFjV9aDNGoxac5d0EPDbiNgpaV/goxRfkroWY9mKiEuASxqKnwGOabLu68AZ3YjLrFWtNMscDFyb2t3fAtwcET+Q9DiwTtJXgAfZsxbznVSLeYni/mDrgnmNX2ytPrmkSMysbK3cLfMIcGSTctdizMwqyj1Uzcwy5ORuZpYhJ3czswy1dJ+7ta7xS00zszK45m5mliEndzOzDDm5m5llyMndzCxDTu5mZhlycjczy5CTu5lZhpzczcwy5ORuZpYhJ3czsww5uZuZZcjJ3cwsQ07uZmYZcnI3M8uQk7uZWYac3M3MMuTkbmaWISd3syYkzZR0i6QnJT0h6YOSDpC0QdJT6eestK4kXSmpX9Ijko4qO34zJ3ez5q4A/iki3gO8H3gCWAXcGRHzgTvTPMBJwPz0WgFc1f1wzfbk5G7WQNL+wJ8BawEi4jcRsRNYClybVrsWOC1NLwWui8K9wExJB3c5bLM9eIDsjDUbrHvL6pNLiGTSORT4BfD3kt4PbAIuAHoiYnta5zmgJ03PAbbWbb8tlW2vK0PSCoqaPT09PfT19TV98927dw+7rCwjxbRyweAe863E3rjNWLfr2beYrtLnVLXz5uRuNtR04CjgLyJio6QreLMJBoCICEkxlp1GxBpgDcDChQujt7e36Xp9fX0Mt6wsI8V0TkMlYsvZzdcbaZuxbrdywSCXbZ7e0jbdUrXz5mYZs6G2AdsiYmOav4Ui2T9fa25JP3ek5QPAIXXbz01lZqVxcjdrEBHPAVslHZ6KFgGPA+uBZalsGXBbml4PfCrdNXMcsKuu+casFG6WMWvuL4DrJb0VeAY4l6IydLOk5cCzwMfTurcDS4B+4LW0rlmpnNzNmoiIh4CFTRYtarJuAOd1PCizMXCzjJlZhpzczcwy5ORuZpahUZO7pEMk3S3pcUmPSboglfs5G2ZmFdXKF6qDwMqIeEDSfsAmSRuAcyies7Fa0iqKTh4XsedzNo6leM7GsZ0Ivtsae3xes3hGSZGYmY1s1Jp7RGyPiAfS9C8pHqA0Bz9nw8ysssZ0K6SkecCRwEa69JyNKml8HkazZ0k0e2ZGlTTGW7XnYYxHDsdg1m4tJ3dJbwe+C3w2Il6R9MayTj5no0oan4dxzeIZQ54l0eyZGVXS+CyOqj0PYzxyOAazdmvpbhlJe1Ek9usj4nup2M/ZMDOrqFbulhHFc62fiIiv1S3yczbMzCqqlWaZ44FPApslPZTKvgCsxs/ZMDOrpFGTe0T8BNAwi/2cDTOzCnIPVTOzDDm5m5llyMndzCxDTu5mZhlycjczy5CTu5lZhpzczcwy5ORuZpYhD5A9xW0e2LXHw862rD65xGjMrF2c3M1sSmkcdAfyrNS4WcbMLENO7mbDkDRN0oOSfpDmD5W0MY0PfJOkt6byvdN8f1o+r8y4zcDJ3WwkF1AMK1lzKXB5RBwGvAwsT+XLgZdT+eVpPbNSObmbNSFpLnAy8O00L+AE4Ja0SuO4wbXxhG8BFql+qDKzEvgLVbPmvg58HtgvzR8I7IyI2iC5tbGBoW7c4IgYlLQrrf9C/Q5bHTe4W2PCbh7YNaRswZz9m647UkyN4wa3EnuzsYbHsl3PvsX0eD6n8b73aKo2lq+Tu1kDSacAOyJik6Tedu231XGDuzUmbLPxfhvH2K0ZKabG/Qy3j/G+d7PtVi4Y5LLN01vapl3vPZqqjeXr5G421PHAqZKWAPsA7wCuAGZKmp5q7/VjA9fGDd4maTqwP/Bi98M2e5Pb3M0aRMTFETE3IuYBZwJ3RcTZwN3A6Wm1xnGDa+MJn57Wjy6GbDaEk7tZ6y4CLpTUT9GmvjaVrwUOTOUXAqtKis/sDW6WMRtBRPQBfWn6GeCYJuu8DpzR1cDMRuGau5lZhlxzH0GzZ1CYmU0GrrmbmWXIyd3MLENO7mZmGXJyNzPLkJO7mVmGnNzNzDLk5G5mliEndzOzDDm5m5llyMndzCxDTu5mZhkaNblLulrSDkmP1pUdIGmDpKfSz1mpXJKuTKPAPyLpqE4Gb2ZmzbVSc78GWNxQtgq4MyLmA3fy5vOrTwLmp9cK4Kr2hGlmZmMxanKPiHuAlxqK60d7bxwF/roo3EsxLNnB7QrWzMxaM95H/vZExPY0/RzQk6bfGAU+qY0Qv50GrY4EX6Zmo6TXazba+WjblK0x3too8jXfuP42Gi2Ys3+nw5qQqo06b1YFE36ee0SEpDGPF9nqSPBlajZKer1rFs8YMtr5aNuUrXGU929cfxuXbR75MmjHyPCdVLVR582qYLx3yzxfa25JP3ek8too8DX1I8SbmVmXjLfmXhvtfTVDR4E/X9I64FhgV13zTaV51CUzy8moyV3SjUAvMFvSNuASiqR+s6TlwLPAx9PqtwNLgH7gNeDcDsRsZmajGDW5R8RZwyxa1GTdAM6baFBmZjYx7qFqZpYhJ3ezBpIOkXS3pMclPSbpglTuntk2aTi5mw01CKyMiCOA44DzJB2Be2bbJOLkbtYgIrZHxANp+pfAExSd8dwz2yaNCXdiMsuZpHnAkcBGJtgzu9Ve2d3qcdusN/V4YmrcTyuxj+W9m21X61k9ns9pvO89mqr1lHZyt1E19gHYsvrkkiLpLklvB74LfDYiXpH0xrLx9MxutVd2t3rcNutNPVxv5JFiatxPKz2ax/LezbZbuWCQyzZPH1fv6fG+92iq1lPazTJmTUjaiyKxXx8R30vF7pltk4aTu1kDFVX0tcATEfG1ukW1ntkwtGf2p9JdM8cxiXpmW77cLGM21PHAJ4HNkh5KZV/APbNtEnFyN2sQET8BNMxi98y2ScHNMmZmGXJyNzPLkJO7mVmGpmybu5/fbmY5c83dzCxDTu5mZhlycjczy5CTu5lZhqbsF6pmZq1qdgNG1R+g55q7mVmGXHO3MZuMtZjc+ZxYoymR3H1Pu5lNNW6WMTPLkJO7mVmGnNzNzDLk5G5mliEndzOzDGV5t4zvjjGzqc41dzOzDDm5m5llyMndzCxDWba5W/eN9j2Hu8LbVNT4e9HN3wPX3M3MMtSR5C5psaSfSeqXtKoT72FWNb7urUra3iwjaRrwTeCjwDbgp5LWR8Tj7X6vGt/6WH25P7Wwndf95oFdnFP3eeX0OVn3dKLN/RigPyKeAZC0DlgKjCu5O3HnazzntsKJrq3XvdlEKSLau0PpdGBxRPz7NP9J4NiIOL9hvRXAijR7OPCztgbSHbOBF8oOYoJyOoY/jIiDygiglet+DNd8Fc9J1WKqWjxQTkzDXvOl3S0TEWuANWW9fztIuj8iFpYdx0T4GLqn1Wu+isdTtZiqFg9UL6ZOfKE6ABxSNz83lZnlzNe9VUonkvtPgfmSDpX0VuBMYH0H3sesSnzdW6W0vVkmIgYlnQ/8GJgGXB0Rj7X7fSpiUjcrJT6GNmjzdV/68TRRtZiqFg9ULKa2f6FqZmblcw9VM7MMObmbmWXIyb0FkvaRdJ+khyU9JulLqfxQSRtTd/Ob0hdplSZpmqQHJf0gzU+qY5C0RdJmSQ9Juj+VHSBpg6Sn0s9ZZcfZimbH0rBckq5M5+YRSUd1MJbDUxy11yuSPtuwTq+kXXXr/FWHYrla0g5Jj9aVtXSOJS1L6zwlaVkH4/lbSU+m83KrpJnDbDviOe6oiPBrlBcg4O1pei9gI3AccDNwZir/FvAfyo61hWO5ELgB+EGan1THAGwBZjeU/XdgVZpeBVxadpzjPZaG5UuAH6Xr7zhgY5fimgY8R9FBpr68t3bddPj9/ww4Cnh0LOcYOAB4Jv2claZndSieE4HpafrS4a650c5xJ1+uubcgCrvT7F7pFcAJwC2p/FrgtBLCa5mkucDJwLfTvJhkxzCMpRSxw+Q9hmaWAtel6+9eYKakg7vwvouApyPi2S681xARcQ/wUkNxK+f4Y8CGiHgpIl4GNgCLOxFPRNwREYNp9l6Kfg2V4uTeotSc8RCwg+KieRrYWXeCtwFzyoqvRV8HPg/8Ps0fyOQ7hgDukLQpdecH6ImI7Wn6OaCnnNDGrNmx1JsDbK2b79b5ORO4cZhlH0zNkz+S9CddiKWmlXNc1uf1aYr/sJoZ7Rx3jAfraFFE/A74QGpbuxV4T8khjYmkU4AdEbFJUm/Z8UzAhyJiQNI7gQ2SnqxfGBEhabLc3zvkWFItsTTpO5dTgYubLH6Aoqlmt6QlwPeB+d2MD6p1jiV9ERgErh9mldLOsWvuYxQRO4G7gQ9S/Jtc+wNZ9e7mxwOnStoCrKNojrmCyXUMRMRA+rmD4o/sMcDzteaK9HNHeRG2bphjqVfGIw1OAh6IiOcbF0TEK7XmyYi4HdhL0uwOx1PTyjnu6ucl6RzgFODsSA3sjVo4xx3j5N4CSQfVvg2XtC/FM7ufoEjyp6fVlgG3lRPh6CLi4oiYGxHzKP7tvisizmYSHYOkGZL2q01TfKn1KEU3/9qdEZU+hpoRjqXeeuBT6a6Z44BddU0TnXIWwzTJSHpX+p4GScdQ5I8XOxxPTSvn+MfAiZJmpbtpTkxlbSdpMUUT56kR8dow67RyjjunjG9xJ9sLeB/wIPBIOjl/lcr/CLgP6Af+Edi77FhbPJ5e3rxbZtIcQ4r14fR6DPhiKj8QuBN4Cvhn4ICyY53AsXwG+EyaFsUAIE8Dm4GFHY5pBkWy3r+urD6e81OsD1N8ifinHYrjRmA78FuKdvPlw51jYCHw7bptP52u5X7g3A7G00/Rvv9Qen0rrftu4PaRznG3Xn78gJlZhtwsY2aWISd3M7MMObmbmWXIyd3MLENO7mZmGXJyNzPLkJO7mVmG/j+bubHoPphK5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7JRjwdIOFxg3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f2e5788-d296-432f-9f20-4348f15be88c"
      },
      "source": [
        "cnt=0\n",
        "for i in data['cleaned_summary']:\n",
        "    if(len(i.split())<=12):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(data['cleaned_summary']))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9936305732484076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZKD5VOWqFxhC"
      },
      "source": [
        "max_text_len=60\n",
        "max_summary_len=12"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yY0tEJP0FxhI"
      },
      "source": [
        "cleaned_text =np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EwLUH78CFxhg"
      },
      "source": [
        "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RakakKHcFxhl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oRHTgX6hFxhq"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "y8KronV2Fxhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5a068d-28b5-482f-fdc2-4e0de4ae9036"
      },
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 66.60336095764272\n",
            "Total Coverage of rare words: 12.751190380114599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J2giEsF3Fxh3"
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DCbGMsm4FxiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5287c21-ffbb-4afd-a60b-18ae7c114199"
      },
      "source": [
        "x_voc"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5804"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eRHqyBkBFxiJ"
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yzE5OiRLFxiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eaa5381-31ef-40ad-d184-4b271575535c"
      },
      "source": [
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 85.48036331073303\n",
            "Total Coverage of rare words: 26.75099069210211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-fswLvIgFxiR"
      },
      "source": [
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "y_voc  =   y_tokenizer.num_words +1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pR8IX9FRFxiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae12b68-b7cb-424c-8592-1c86f168f875"
      },
      "source": [
        "y_tokenizer.word_counts['sostok'],len(y_tr)   "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3931, 3931)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kZ-vW82sFxih"
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cx5NISuMFxik"
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence to Sequence Model Building"
      ],
      "metadata": {
        "id": "1PG7S0y14hWX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zXef38nBFxir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790a2516-4f48-4da3-a275-ebd849c695b6"
      },
      "source": [
        "from tensorflow.keras import backend as K \n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary() "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 60)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 60, 100)      580400      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 60, 300), (N 481200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 60, 300), (N 721200      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    113600      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 60, 300), (N 721200      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 1136)   682736      concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 3,961,836\n",
            "Trainable params: 3,961,836\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Lwfi1Fm8Fxiz"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-A3J92MUljB"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ETnPzA4OFxi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187d65f9-9db8-49c1-e3b3-da07376dc274"
      },
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=5,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 3929 samples, validate on 437 samples\n",
            "Epoch 1/5\n",
            "3929/3929 [==============================] - 194s 49ms/sample - loss: 4.2919 - val_loss: 3.6654\n",
            "Epoch 2/5\n",
            "3929/3929 [==============================] - 169s 43ms/sample - loss: 3.7788 - val_loss: 3.4705\n",
            "Epoch 3/5\n",
            "3929/3929 [==============================] - 187s 48ms/sample - loss: 3.6070 - val_loss: 3.3654\n",
            "Epoch 4/5\n",
            "3929/3929 [==============================] - 169s 43ms/sample - loss: 3.5194 - val_loss: 3.3286\n",
            "Epoch 5/5\n",
            "3929/3929 [==============================] - 166s 42ms/sample - loss: 3.4498 - val_loss: 3.2777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tDTNLAURFxjE"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sBX0zZnOFxjW"
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9QkrNV-4Fxjt"
      },
      "source": [
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6f6TTFnBFxj6"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "  \n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "  \n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "  \n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "  \n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aAUntznIFxj9"
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BUtQmQTmFxkI"
      },
      "source": [
        "for i in range(0,100):\n",
        "    print(\"Review:\",seq2text(x_tr[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}